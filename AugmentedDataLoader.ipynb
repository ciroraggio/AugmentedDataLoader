{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Condizioni iniziali:\n",
    "1. Ho delle immagini nrrd di **N** pazienti\n",
    "2. Definisco una lista di **M** trasformazioni MONAI per effettuare l'augmentation di queste immagini (Rotazione, scaling eccetera)\n",
    "3. Impongo un batch size uguale a **K**\n",
    "4. Impongo che deve leggere **J** pazienti per volta dal disco\n",
    "\n",
    "## Procedimento:\n",
    "1. Leggo J pazienti tra gli N che ho, in modo casuale\n",
    "2. Applico a tutti i J pazienti le M trasformazioni, arrivando così a **(J*M)+J** immagini (J pazienti non aumentati + (J*M) aumentati)\n",
    "3. Faccio lo shuffle delle (J*M)+J immagini\n",
    "4. Fornisco i (J*M)+J  casi che ho ottenuto a blocchi di K \n",
    "5. Quando ho passato tutti i (J*M)+J pazienti a blocchi di K, leggo altri J pazienti e ripeto dal punto 2 fino a quando non ho letto tutti gli N pazienti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from monai.data import ImageDataset\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\"\"\"\n",
    "Inizializza il dataset e genera i batch utilizzando il metodo 'generate_batches'. \n",
    "Dopo aver creato un'istanza di AugmentedDataLoader con i parametri necessari, utilizzare il metodo generate_batches() per iterare sui batch di dati.\n",
    "    - dataset -> Dataset di tipo ImageDataset, contiene: immagini, etichette e trasformazioni sistematiche\n",
    "    - augmentation_transforms -> Lista di (M) trasformazioni MONAI per l'augmentation\n",
    "    - batch_size -> Dimensione del batch (K) ovvero i blocchi da restituire\n",
    "    - num_patients -> Numero totale di pazienti (N)\n",
    "    - subset_len -> Lunghezza del subset (J)\n",
    "    - [optional] debug_path -> se indicato, è il path dove l'utente sceglie di salvare una fetta dell'immagine, per ogni immagine dei batch restituiti \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AugmentedDataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: ImageDataset,\n",
    "        augmentation_transforms: list,\n",
    "        batch_size: int,\n",
    "        subset_len: int,\n",
    "        debug_path: str = None,\n",
    "    ):\n",
    "        self.dataset = dataset  # Dataset di tipo ImageDataset, contiene: immagini, etichette e trasformazioni sistematiche\n",
    "        self.augmentation_transforms = augmentation_transforms  # Lista di (M) trasformazioni MONAI per l'augmentation\n",
    "        self.batch_size = batch_size  # Dimensione del batch (K)\n",
    "        self.num_patients = len(dataset.image_files)  # Numero totale di pazienti (N)\n",
    "        self.subset_len = subset_len  # Lunghezza del subset (J)\n",
    "        self.debug_path = debug_path  # se indicato, è il path dove l'utente sceglie di salvare una fetta dell'immagine, per ogni immagine dei batch restituiti\n",
    "\n",
    "    def generate_batches(self):\n",
    "        if self.dataset is None:\n",
    "            raise Exception(\"Dataset is None\")\n",
    "\n",
    "        if (\n",
    "            self.dataset.image_files\n",
    "            and self.dataset.seg_files\n",
    "            and len(self.dataset.image_files) != len(self.dataset.seg_files)\n",
    "        ):\n",
    "            raise Exception(\"The length of the images and segmentations don't match\")\n",
    "\n",
    "        # Creo una lista contenente tutti gli indici dei pazienti ed applico lo shuffle per non operare sui pazienti nello stesso ordine di arrivo\n",
    "        shuffle_patient_indices = list(range(self.num_patients))\n",
    "        random.shuffle(shuffle_patient_indices)\n",
    "\n",
    "        index = 0\n",
    "        while index < self.num_patients:\n",
    "            \"\"\"\n",
    "            ***checked_subset_len*** determina la lunghezza del subset che sarà estratto,\n",
    "            se il valore specificato dall'utente per subset_len è maggiore del numero di pazienti rimanenti,\n",
    "            allora subset_len sarà pari al numero di pazienti rimanenti, in modo da evitare di superare la lunghezza della lista dei pazienti\n",
    "            \"\"\"\n",
    "            remaining_patients = self.num_patients - index\n",
    "            checked_subset_len = min(self.subset_len, remaining_patients)\n",
    "\n",
    "            \"\"\"\n",
    "            Viene creato un subset di pazienti (che vengono scelti dalla lista di indici precedentemente mischiati) avente lunghezza J\n",
    "            \"\"\"\n",
    "            subset_indices = shuffle_patient_indices[index : index + checked_subset_len]\n",
    "            subset = torch.utils.data.Subset(self.dataset, subset_indices)\n",
    "\n",
    "            augmented_subset = []\n",
    "            for data in subset:\n",
    "                augmented_data = []\n",
    "                for transformation in self.augmentation_transforms:\n",
    "                    augmented_data.append(transformation(data[0]))\n",
    "\n",
    "                augmented_subset.append(data[0])  # Aggiungo le immagini NON aumentate\n",
    "                augmented_subset.extend(\n",
    "                    augmented_data\n",
    "                )  # Aggiungo le immagini aumentate\n",
    "\n",
    "            \"\"\"\n",
    "            Ulteriore shuffle delle (J*M)+J immagini, in questo modo riceverà immagini aumentate e non aumentate mixate\n",
    "            \"\"\"\n",
    "            full_batch_indices = torch.randperm(len(augmented_subset))\n",
    "            augmented_subset = [augmented_subset[idx] for idx in full_batch_indices]\n",
    "\n",
    "            \"\"\"\n",
    "            Creo blocchi di dimensione K\n",
    "            - range(0, len(augmented_subset), self.batch_size): \n",
    "                genera una sequenza di valori che rappresentano gli indici di inizio di ogni blocco. \n",
    "                Gli indici partono da 0 e avanzano con un passo pari a self.batch_size, fino a raggiungere la lunghezza totale di augmented_subset.\n",
    "            - augmented_subset[i:i + self.batch_size]: \n",
    "                seleziona una sotto-lista di augmented_subset che va dall'indice i fino all'indice i + self.batch_size. \n",
    "                Questo crea un blocco di immagini di dimensione self.batch_size.\n",
    "            \"\"\"\n",
    "            blocks = [\n",
    "                augmented_subset[i : i + self.batch_size]\n",
    "                for i in range(0, len(augmented_subset), self.batch_size)\n",
    "            ]\n",
    "            \n",
    "            image_count = 0\n",
    "            for block in blocks:\n",
    "                \"\"\"\n",
    "                Restituisco i blocchi di dimensione K.\n",
    "                Per ogni blocco, viene creato un tensore batch utilizzando la funzione stack di torch che concatena i tensori all'interno del blocco lungo la dimensione 0,\n",
    "                creando così un unico tensore che rappresenta un batch di immagini.\n",
    "                Operatore yield per mantenere lo stato della funzione tra le chiamate.\n",
    "                \"\"\"\n",
    "                if self.debug_path:\n",
    "                    for i, data in enumerate(block):\n",
    "                        image = data[0]\n",
    "                        central_slice = image[image.shape[0] // 2] # Estraggo la fetta centrale sul primo canale\n",
    "                        debug_image_path = os.path.join(self.debug_path, f\"augmented_image_{image_count}.png\")\n",
    "                        vutils.save_image(central_slice, debug_image_path)\n",
    "                        image_count += 1\n",
    "\n",
    "                batch = torch.stack(block)\n",
    "                yield batch\n",
    "\n",
    "            \"\"\"\n",
    "            Dopo aver passato tutti i (J*M)+J pazienti a blocchi di K, incremento l'indice e riparto per leggere altri J pazienti\n",
    "            \"\"\"\n",
    "            index += checked_subset_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor or list of tensors expected, got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m data_loader \u001b[39m=\u001b[39m AugmentedDataLoader(dataset, augmentation_transforms, batch_size, \u001b[39m2\u001b[39m, debug_path)\n\u001b[0;32m     31\u001b[0m blocks \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfor\u001b[39;00m batch_data \u001b[39min\u001b[39;00m data_loader\u001b[39m.\u001b[39mgenerate_batches():\n\u001b[0;32m     33\u001b[0m     \u001b[39m# inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m         \u001b[39m# print(f\"Batch data shape: {batch_data.shape}\")\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         inputs, labels \u001b[39m=\u001b[39m (batch_data[\u001b[39m0\u001b[39m], \u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch_data) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m batch_data\n",
      "Cell \u001b[1;32mIn[3], line 109\u001b[0m, in \u001b[0;36mAugmentedDataLoader.generate_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m         image \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()  \u001b[39m# Assume che l'immagine sia il primo elemento della tupla\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         debug_image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maugmented_image_\u001b[39m\u001b[39m{\u001b[39;00mimage_count\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m         vutils\u001b[39m.\u001b[39;49msave_image(image, debug_image_path)\n\u001b[0;32m    110\u001b[0m         image_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    112\u001b[0m batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(block)\n",
      "File \u001b[1;32mc:\\Users\\CiroRaggio\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CiroRaggio\\anaconda3\\lib\\site-packages\\torchvision\\utils.py:147\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    146\u001b[0m     _log_api_usage_once(save_image)\n\u001b[1;32m--> 147\u001b[0m grid \u001b[39m=\u001b[39m make_grid(tensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    148\u001b[0m \u001b[39m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[0;32m    149\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\CiroRaggio\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CiroRaggio\\anaconda3\\lib\\site-packages\\torchvision\\utils.py:63\u001b[0m, in \u001b[0;36mmake_grid\u001b[1;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensor or list of tensors expected, got a list containing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(t)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensor or list of tensors expected, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[39m# if list of tensors, convert to a 4D mini-batch Tensor\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, \u001b[39mlist\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: tensor or list of tensors expected, got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "from monai.transforms import Rotate, Compose, Resize\n",
    "\n",
    "# ImageDataset params\n",
    "images_to_transform = [\n",
    "    \"./data/PDDCA-1.4.1_part1/0522c0001/img.nrrd\",\n",
    "    \"./data/PDDCA-1.4.1_part1/0522c0002/img.nrrd\",\n",
    "    \"./data/PDDCA-1.4.1_part1/0522c0003/img.nrrd\",\n",
    "    # \"./data/PDDCA-1.4.1_part1/0522c0009/img.nrrd\",\n",
    "    # \"./data/PDDCA-1.4.1_part1/0522c0013/img.nrrd\",\n",
    "]\n",
    "\n",
    "labels_to_transform = [\n",
    "    \"./data/PDDCA-1.4.1_part1/0522c0001/structures/Parotid_L.nrrd\",\n",
    "    \"./data/PDDCA-1.4.1_part1/0522c0002/structures/Parotid_L.nrrd\",\n",
    "    \"./data/PDDCA-1.4.1_part1/0522c0003/structures/Parotid_L.nrrd\",\n",
    "    # \"./data/PDDCA-1.4.1_part1/0522c0009/structures/BrainStem.nrrd\",\n",
    "    # \"./data/PDDCA-1.4.1_part1/0522c0013/structures/BrainStem.nrrd\",\n",
    "]\n",
    "each_image_trans = Compose([Resize([74,74,74])])\n",
    "\n",
    "# AugmentedDataLoader params\n",
    "augmentation_transforms = [\n",
    "    Rotate(angle=35),\n",
    "    Rotate(angle=61),\n",
    "]\n",
    "batch_size = 2\n",
    "num_patients = len(images_to_transform)\n",
    "dataset = ImageDataset(image_files=images_to_transform, labels=labels_to_transform, transform=each_image_trans)\n",
    "debug_path='./data/debug_path_test'\n",
    "data_loader = AugmentedDataLoader(dataset, augmentation_transforms, batch_size, 2, debug_path)\n",
    "blocks = 0\n",
    "for batch_data in data_loader.generate_batches():\n",
    "    # inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "    try:\n",
    "        # print(f\"Batch data shape: {batch_data.shape}\")\n",
    "        inputs, labels = (batch_data[0], None) if len(batch_data) == 1 else batch_data\n",
    "        blocks+=1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore: {e}\")\n",
    "\n",
    "print(f\"Totale immagini: {num_patients}\")\n",
    "print(f\"Totale trasformazioni: {len(augmentation_transforms)}\")\n",
    "print(f\"Grandezza batch richiesta: {batch_size}\")\n",
    "print(f\"Blocchi ricevuti: {blocks}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
